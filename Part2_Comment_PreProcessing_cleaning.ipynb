{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part2 Fresh and Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob, Word\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import sklearn\n",
    "import glob\n",
    "from textblob import TextBlob, Word\n",
    "import langdetect\n",
    "import unicodedata\n",
    "import re\n",
    "import contractions\n",
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', parse=False, tag=False, entity=False)\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('but')\n",
    "stop_words.remove('not')\n",
    "special_char_pattern = re.compile(r'([{.(-)!}])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importdata(path):\n",
    "    all_files = glob.glob(path + \"/*.csv\")\n",
    "    li = []\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        li.append(df)\n",
    "    frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "    del frame['Unnamed: 0']\n",
    "    frame = frame.dropna()\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', str(text)).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_frase(text):\n",
    "    return nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_parola(text):\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rilevo_filtro_lingua(text):\n",
    "    try:\n",
    "        if detect(text):\n",
    "            return detect(text)\n",
    "        else:\n",
    "            return (\"Not_detected\") \n",
    "    except Exception as e:\n",
    "        return (\"Not_detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rimuovo_newline(text):\n",
    "    return text.translate(text.maketrans(\"\\n\\t\\r\", \"   \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rimuovo_caratteri_speciali(text):\n",
    "    return remove_special_characters(special_char_pattern.sub(\" \\\\1 \", text), remove_digits=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_parola(text, stemmer=ps):\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPEAT_PATTERN = re.compile(r'(\\w*)(\\w)\\2')\n",
    "MATCH_SUBSTITUTION = r'\\1\\2'\n",
    "\n",
    "def remove_repeated_characters(word):\n",
    "    if wordnet.synsets(word):\n",
    "        return word\n",
    "    new_word = REPEAT_PATTERN.sub(MATCH_SUBSTITUTION, word)\n",
    "    return (remove_repeated_characters(new_word) \n",
    "               if new_word != word else new_word)\n",
    "\n",
    "def duplicate_char_remover(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    final_sentence = ' '.join(remove_repeated_characters(word.lower()) \n",
    "                                  for word in tokens)\n",
    "    return final_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False, stopwords=None):\n",
    "    if not stopwords:\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    \n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    \n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment= importdata(\"/Users/simone/Documents/Master_scraping/file_scrpaing/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment['comment'] = df_comment['comment'].apply(lambda x: x.lower()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment['comment'] = df_comment['comment'].apply(lambda x: rimuovo_newline(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment['no_accent'] = df_comment['comment'].apply(lambda x: remove_accented_chars(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment['no_duplicate'] = df_comment['no_accent'].apply(lambda x: duplicate_char_remover(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment['no_contracted'] = df_comment['no_duplicate'].apply(lambda x: expand_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment['no_special'] = df_comment['no_contracted'].apply(lambda x: rimuovo_caratteri_speciali(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment['language'] = df_comment['no_special'].apply(lambda x: rilevo_filtro_lingua(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment = df_comment.loc[df_comment['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment['stemming'] = df_comment['no_special'].apply(lambda x: stemming_parola(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment['no_stop_words'] = df_comment['stemming'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment.to_csv(\"/Users/simone/Documents/Master_scraping/output/preprocessed_comment.csv\",sep=\"\\t\",header=True,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
